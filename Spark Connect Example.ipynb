{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d8ad61-8045-40dc-aff7-12f709d75e49",
   "metadata": {},
   "source": [
    "## Spark Connect Example\n",
    "### Prerequisites:\n",
    "- Install Spark\n",
    "- Install pyspark with pip or conda\n",
    "- Install these packages:\n",
    "    pandas\n",
    "    pyarrow\n",
    "    grpcio\n",
    "    google-api-python-client\n",
    "    grpcio-status\n",
    "\n",
    "### Start the Spark Connect server \n",
    "(https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_connect.html)\n",
    "- Find the script `start-connect-server.sh` (in the sbin folder of your Spark installation) and run\n",
    "  ` <PATH OF YOUR SPARK FOLDER>/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.5.3` (you might need to adjust the version number)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775bf900-adfa-4a3d-9a83-bbecf8740d92",
   "metadata": {},
   "source": [
    "### Example \n",
    "from https://spark.apache.org/docs/latest/spark-connect-overview.html\n",
    "\n",
    "If you everything works correctly you only need to change the name of the log file and the following code should run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e989a",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "In case you're receiving an error similar to the following, check your java version using `java -version`. For me version 22 did not work, so I had to downgrade to version 17. Make sure to update your JAVA_HOME env variable accordingly. \n",
    "\n",
    "```\n",
    "SparkConnectGrpcException: (org.apache.spark.SparkException) Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 9) (172.16.79.232 executor driver): java.lang.UnsupportedOperationException: sun.misc.Unsafe or java.nio.DirectByteBuffer.<init>(long, int) not available\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be1b2a0-8b8c-4380-8453-368451bcf2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.remote(\"sc://localhost\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "757be3a6-e8cc-4fbe-94bc-acefcc25b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 1, lines with b: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"SimpleApp.py\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "logFile = \"<SOME_TEXT_FILE>.txt\"  # Should be some file on your system\n",
    "spark = SparkSession.builder.remote(\"sc://localhost:4041\").appName(\"SimpleApp\").getOrCreate()\n",
    "logData = spark.read.text(logFile).cache()\n",
    "\n",
    "numAs = logData.filter(logData.value.contains('a')).count()\n",
    "numBs = logData.filter(logData.value.contains('b')).count()\n",
    "\n",
    "print(\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
